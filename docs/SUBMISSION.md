# SUBMISSION

## Project

- Name: `native`
- Tagline: `The one app you need to become a local anywhere.`

## Problem statement fit

1. Multilinguality:
   - Realtime speech and visual-text understanding across Indian languages.
2. Localization:
   - Helps users understand local forms, announcements, and conversations.
3. Consumer:
   - Addresses daily friction for internal migrants navigating new cities.

## What is unique

`native` combines seeing + hearing + search grounding + reasoning + speech output in one continuous app workflow.

## Demo format for judges (3 minutes)

- 40s: Problem and user story (internal migrants).
- 2m: Scene A + Scene B + Scene C live demo.
- 20s: Why Gemini Live API enables this integrated experience.

## One-minute video structure

- 15s: Problem statement.
- 35s: Product in action (3 scenes quick cuts).
- 10s: Impact close.

## Submission checklist

- [ ] Public repo link added.
- [ ] 1-minute demo video link added.
- [ ] Brief description pasted into form.
- [ ] Team member details complete.

## Placeholders to fill before submit

- Repo: `https://github.com/DivyamJindal/native-deepmind-hackathon`
- Video: `ADD_VIDEO_LINK_HERE`
- Submission URL: `https://cerebralvalley.ai/e/gemini-3-bengaluru-hackathon/hackathon/submit`

## Q/A cheat sheet

Q: `Is this just translation?`
A: No. It is realtime multimodal localization: camera context + ambient audio + grounding + stepwise guidance in one flow.

Q: `How is this different from existing tools?`
A: Existing tools solve isolated tasks (text OCR or conversation translation). `native` integrates four tasks with one live interaction loop.
